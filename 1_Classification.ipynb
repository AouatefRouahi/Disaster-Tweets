{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab94bd61-d958-47c8-84e2-ce55b81b54ce",
   "metadata": {},
   "source": [
    "# Disaster Tweets\n",
    "-------------------------------------------------------\n",
    ">In this second phase of the project, we will:  \n",
    ">> define and train the classification model based on the **preprocessed tweets**.\n",
    "\n",
    "-------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f98003",
   "metadata": {},
   "source": [
    "# Import useful Librairies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8331ed27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-24 21:42:13.851155: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-04-24 21:42:13.851223: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Machine learning librairies\n",
    "import tensorflow as tf\n",
    "\n",
    "# global params\n",
    "pre_file_path = \"data/pre_train.csv\"\n",
    "models_path = 'models/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9449b019",
   "metadata": {},
   "source": [
    "# Load Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58d0254b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>word_count</th>\n",
       "      <th>unique_word_count</th>\n",
       "      <th>stop_word_count</th>\n",
       "      <th>url_count</th>\n",
       "      <th>char_count</th>\n",
       "      <th>punctuation_count</th>\n",
       "      <th>hashtag_count</th>\n",
       "      <th>at_count</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>clean_keyword</th>\n",
       "      <th>keyword_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>69</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>deed reason earthquake allah forgive</td>\n",
       "      <td>NaN</td>\n",
       "      <td>deed reason earthquake allah forgive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>forest fire near ronge sask canada</td>\n",
       "      <td>NaN</td>\n",
       "      <td>forest fire near ronge sask canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>20</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>133</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>resident ask shelter place notify officer evac...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>resident ask shelter place notify officer eva...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>65</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>people receive wildfire evacuation order calif...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>people receive wildfire evacuation order cali...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>15</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>88</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>got send photo ruby alaska smoke wildfire pour...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>got send photo ruby alaska smoke wildfire pou...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  keyword                                               text  target  \\\n",
       "0     NaN  Our Deeds are the Reason of this #earthquake M...       1   \n",
       "1     NaN             Forest fire near La Ronge Sask. Canada       1   \n",
       "2     NaN  All residents asked to 'shelter in place' are ...       1   \n",
       "3     NaN  13,000 people receive #wildfires evacuation or...       1   \n",
       "4     NaN  Just got sent this photo from Ruby #Alaska as ...       1   \n",
       "\n",
       "   word_count  unique_word_count  stop_word_count  url_count  char_count  \\\n",
       "0          13                 13                8          0          69   \n",
       "1           7                  7                0          0          38   \n",
       "2          22                 20               11          0         133   \n",
       "3           8                  8                1          0          65   \n",
       "4          16                 15                7          0          88   \n",
       "\n",
       "   punctuation_count  hashtag_count  at_count  \\\n",
       "0                  1              1         0   \n",
       "1                  1              0         0   \n",
       "2                  3              0         0   \n",
       "3                  2              1         0   \n",
       "4                  2              2         0   \n",
       "\n",
       "                                          clean_text clean_keyword  \\\n",
       "0               deed reason earthquake allah forgive           NaN   \n",
       "1                 forest fire near ronge sask canada           NaN   \n",
       "2  resident ask shelter place notify officer evac...           NaN   \n",
       "3  people receive wildfire evacuation order calif...           NaN   \n",
       "4  got send photo ruby alaska smoke wildfire pour...           NaN   \n",
       "\n",
       "                                        keyword_text  \n",
       "0               deed reason earthquake allah forgive  \n",
       "1                 forest fire near ronge sask canada  \n",
       "2   resident ask shelter place notify officer eva...  \n",
       "3   people receive wildfire evacuation order cali...  \n",
       "4   got send photo ruby alaska smoke wildfire pou...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = pd.read_csv(pre_file_path)\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c57650e",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea15700-2fc5-430e-8e5e-52d38a0a428c",
   "metadata": {},
   "source": [
    "## Prepare data for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf175ea",
   "metadata": {},
   "source": [
    "1) Build the **vocabulary** using Tokenizer of keras  \n",
    "2) Transform the plain texts to sequences of integers while only considering the *k* most common words  \n",
    "3) Tensorify the list of sequences with their classes \n",
    "4) Split the data in train and test sets and generate the batchs\n",
    "\n",
    "> Parameters :   \n",
    ">> **_max_features_**: max number of words to take into account for the model training   \n",
    ">> **_train_ratio_**: used to split the data into training and validation sets        \n",
    ">> **_batch_size_** = size of the batches    \n",
    ">> **_seq_length_** = the length of the integer sequences. It is induced from max_features   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ae9037-23ca-43f8-b384-150cfc691fe8",
   "metadata": {},
   "source": [
    "###  Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03a6436a-9dcf-4721-8754-5b29f73eeb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 5_000\n",
    "train_ratio = 0.8     \n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104178ef-69f3-4e2e-b8c4-144efd1a5598",
   "metadata": {},
   "source": [
    "###  Build Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bac5ebb5-6145-42e4-9fd2-35b037adca0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size : 14378\n"
     ]
    }
   ],
   "source": [
    "#build the vocab and keep the K most common word based on word frequency (K = max_features)\n",
    "# max_features+ 1(1 OOV token)\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words = max_features + 1)\n",
    "tokenizer.fit_on_texts(tweets[\"keyword_text\"])\n",
    "print(f'vocabulary size : {len(tokenizer.word_counts)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c925c9-7b37-4831-ae2f-ab179c0c87c2",
   "metadata": {},
   "source": [
    "###  Transform text to Integer Sequences of equal lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9682e86-c8aa-4cef-ac2f-a157404196a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3491,  457,   94, ...,    0,    0,    0],\n",
       "       [ 195,    1,  245, ...,    0,    0,    0],\n",
       "       [1356,  497, 1668, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [3254,  239, 1235, ...,    0,    0,    0],\n",
       "       [  18,  751, 1800, ...,    0,    0,    0],\n",
       "       [ 177,   46,  179, ...,    0,    0,    0]], dtype=int32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transforms each text to a sequence of integers, only the K most common words will be transformed (K = max_features)\n",
    "tweets[\"tweet_encoded\"] = tokenizer.texts_to_sequences(tweets.keyword_text)\n",
    "\n",
    "# check whether we have empty lists\n",
    "tweets['length'] = tweets['tweet_encoded'].apply(lambda x : len(x))\n",
    "tweets = tweets[tweets[\"length\"]!=0]\n",
    "\n",
    "# add padding so that all sequences have the same length --> a numpy array of equal length sequences\n",
    "tweet_pad = tf.keras.preprocessing.sequence.pad_sequences(tweets.tweet_encoded, padding=\"post\")\n",
    "tweet_pad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe57ea42-10d1-4c97-b1ec-22d46855e713",
   "metadata": {},
   "source": [
    "###  Tensorify the Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6414f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-24 21:42:15.945984: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-04-24 21:42:15.946037: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-04-24 21:42:15.946063: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (jupyter-rouahi-2eaouatef-40gmail-2ecom): /proc/driver/nvidia/version does not exist\n",
      "2022-04-24 21:42:15.946421: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([3491,  457,   94, 1246, 1667,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0], dtype=int32),\n",
       " 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform the data to a tensor (TensorSliceDataset)\n",
    "full_ds = tf.data.Dataset.from_tensor_slices((tweet_pad, tweets.target.values))\n",
    "list(full_ds.as_numpy_iterator())[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042b3c6a-a552-47b2-bcd1-a80bfffe124c",
   "metadata": {},
   "source": [
    "###  Training and Validation sets & Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93dccecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 219  219  292  412    1  453  210   66 2278 2279  832    0    0    0\n",
      "     0    0    0    0    0    0]\n",
      " [  26 1921 3191  776  344   26  303   29  483  540    2 3199   75    0\n",
      "     0    0    0    0    0    0]\n",
      " [ 261 1979  598  374  887  261 2354    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0]\n",
      " [ 437    2  115 2907    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0]\n",
      " [  51   51  114 2439 3079 4129 2124 4130    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0]\n",
      " [ 383 1116 1659    7 1242    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0]\n",
      " [  89 2539   22  928   89    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0]\n",
      " [ 184  959  568  856  165  297  184    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0]\n",
      " [  34 1106 2239 1393 1339 1030 1354  865  993  760    0    0    0    0\n",
      "     0    0    0    0    0    0]\n",
      " [ 163 2072 1106  312  163    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0]\n",
      " [  97  165   97  578  946 1387 4554  264  348    0    0    0    0    0\n",
      "     0    0    0    0    0    0]\n",
      " [  34 3625 1300   34  231  471    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0]\n",
      " [ 144  144  379  420  308 1232  367  683   11 1078  896 2204    0    0\n",
      "     0    0    0    0    0    0]\n",
      " [ 400 4901  279 1082 3190 1651  332  314   15    0    0    0    0    0\n",
      "     0    0    0    0    0    0]\n",
      " [ 174 1408  794 4435  174    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0]\n",
      " [ 400  314   15  535  287    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0]\n",
      " [  32   60  211   53   32    5    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0]\n",
      " [ 110    7  802  188  110  228  772    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0]\n",
      " [  13  280  256   13 4362 2044    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0]\n",
      " [  79 3102 1300  538  591 1075   79 1127    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0]\n",
      " [   9  302 1044  355   47    9    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0]\n",
      " [ 221  305  284 2502   93 4155  543 1449  221  503  235    0    0    0\n",
      "     0    0    0    0    0    0]\n",
      " [ 438 3841   20 1573  588  717 1988  476   30 1304    2  823    0    0\n",
      "     0    0    0    0    0    0]\n",
      " [ 143   47  482   30  570   76 3677 2919  942 1357 2920 2920    0    0\n",
      "     0    0    0    0    0    0]\n",
      " [  59  569  890  115 1224 2328  182 4150   59    0    0    0    0    0\n",
      "     0    0    0    0    0    0]\n",
      " [ 252 2020 1757  252    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0]\n",
      " [ 383 1471  825   82  508   29 1071   19    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0]\n",
      " [  32 1423    1  578   24   85   84   32    2  643  455  264    0    0\n",
      "     0    0    0    0    0    0]\n",
      " [   4 1742    4   47    1 1000 1134  932   46  114 2437  474  270    0\n",
      "     0    0    0    0    0    0]\n",
      " [ 101 2810  101    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0]\n",
      " [  56   19 1285 1129 1191   56  259  274  819    0    0    0    0    0\n",
      "     0    0    0    0    0    0]\n",
      " [  35 1481 1482 2393   35  485 2568 1481 1482  976    0    0    0    0\n",
      "     0    0    0    0    0    0]], shape=(32, 20), dtype=int32) tf.Tensor([1 1 1 0 0 0 0 1 1 1 0 1 1 1 0 0 1 1 0 0 0 1 0 0 0 0 0 1 0 1 0 0], shape=(32,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# Train Test Split\n",
    "train_size = int(train_ratio * tweets.shape[0])\n",
    "\n",
    "train_data = full_ds.take(train_size).shuffle(train_size).batch(batch_size)\n",
    "test_data = full_ds.skip(train_size).batch(batch_size)\n",
    "\n",
    "for tweet, label in train_data.take(1):\n",
    "    print(tweet, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eefe0769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# features input_shape = (bs, seq_len)\n",
    "# target input_shape = (bs,) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11054687-46d8-428c-83d4-7beedbb6f2ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of train batches : 191\n"
     ]
    }
   ],
   "source": [
    "print('number of train batches :', len(train_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b863a41",
   "metadata": {},
   "source": [
    "## baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7bccd1-4a61-4a71-b67e-43fe3ec0b9d8",
   "metadata": {},
   "source": [
    ">ðŸ—’ A **baseline** model (**dummy** predictor) allows us to set a **lower bound** on performance for model evaluation. We expect that a trained model outperforms this baseline model.\n",
    "\n",
    "> Given the marginal probabilities **_p(y=1)=r_** and **_p(y=0)=1âˆ’r_**, assume that the baseline model **always predicts the majority class** . \n",
    ">> Baseline **Accuracy = max (r, 1-r)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1266c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_1 = 0.4298291721419185\n",
      "p_0 = 0.5701708278580815\n",
      "Baseline Accuracy Score = 0.57\n"
     ]
    }
   ],
   "source": [
    "# baseline_model : always predicts the majority class\n",
    "p_1 = tweets[tweets['target']==1].shape[0] / tweets.shape[0]\n",
    "p_0 = tweets[tweets['target']==0].shape[0] / tweets.shape[0]\n",
    "print('p_1 = {}'.format(p_1))\n",
    "print('p_0 = {}'.format(p_0))\n",
    "\n",
    "baseline_accuracy = round(max(p_1, p_0),2)\n",
    "print(f'Baseline Accuracy Score = {baseline_accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f19eaf",
   "metadata": {},
   "source": [
    "## LSTM model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b74662-14da-407a-8bd0-3b04cb29b281",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cbb83b64-d832-4bcd-89ba-fc91a6f17d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_dim = vocab_size + 1 = len(tokenizer.word_index or index_word or word_counts) +1  (+1 for the padding 0)\n",
    "vocab_size = len(tokenizer.word_index)\n",
    "\n",
    "# input_length or input_shape ([seq_len,]) = length of input sequences after padding \n",
    "seq_length = tweet_pad[0].shape[0]\n",
    "\n",
    "# output_dim =  size of the vector space in which words will be embedded\n",
    "\n",
    "# Dropout ratio to avoid overfitting\n",
    "\n",
    "# output_layer activation function = 'sigmoid' for binary classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90269af4-56f7-4833-8b00-ac0428d6e6c9",
   "metadata": {},
   "source": [
    "### Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c511fdfe-3f63-4d42-8f31-4c1fb67e1766",
   "metadata": {},
   "source": [
    "<img src=\"img/lstm.png\" width=\"500\" height=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "065cfe36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 20, 16)            230064    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 20, 16)            0         \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 32)                6272      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 236,369\n",
      "Trainable params: 236,369\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim = vocab_size +1, output_dim = 16, input_length = seq_length),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.LSTM(units = 32, return_sequences=False), # maintains the sequential nature\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    \n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61baa309-a147-4b1e-9a76-458810c48764",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "63778821",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer= tf.keras.optimizers.Adam()\n",
    "\n",
    "model.compile(optimizer = optimizer,\n",
    "              loss = tf.keras.losses.BinaryCrossentropy(),\n",
    "              metrics = [tf.keras.metrics.BinaryAccuracy()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d942de8-fab2-4fed-86e5-ef9937757224",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "38e2d0f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "191/191 [==============================] - 3s 16ms/step - loss: 0.2457 - binary_accuracy: 0.9102 - val_loss: 0.9056 - val_binary_accuracy: 0.6945\n",
      "Epoch 2/10\n",
      "191/191 [==============================] - 3s 18ms/step - loss: 0.2154 - binary_accuracy: 0.9205 - val_loss: 0.8501 - val_binary_accuracy: 0.7017\n",
      "Epoch 3/10\n",
      "191/191 [==============================] - 3s 16ms/step - loss: 0.1979 - binary_accuracy: 0.9315 - val_loss: 0.8864 - val_binary_accuracy: 0.7116\n",
      "Epoch 4/10\n",
      "191/191 [==============================] - 3s 17ms/step - loss: 0.1801 - binary_accuracy: 0.9359 - val_loss: 0.9936 - val_binary_accuracy: 0.6813\n",
      "Epoch 5/10\n",
      "191/191 [==============================] - 3s 17ms/step - loss: 0.1692 - binary_accuracy: 0.9409 - val_loss: 1.2041 - val_binary_accuracy: 0.6656\n"
     ]
    }
   ],
   "source": [
    "es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience = 3)\n",
    "\n",
    "history = model.fit(\n",
    "    train_data,\n",
    "    epochs=10, \n",
    "    validation_data = test_data,\n",
    "    callbacks = [es_callback]        \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "41eeed2c-2c8f-41b4-b429-023154f76aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------------- Train Accuracy ------------------------------\n",
      "\n",
      "Mean:  0.9277923822402954\n",
      "Std:  0.011095163291250277\n",
      "\n",
      "---------------------------- Validation Accuracy ------------------------------\n",
      "\n",
      "Mean:  0.6909329771995545\n",
      "Std:  0.01605950484051095\n"
     ]
    }
   ],
   "source": [
    "print('\\n---------------------------- Train Accuracy ------------------------------\\n')\n",
    "print('Mean: ', np.mean(history.history['binary_accuracy']))\n",
    "print('Std: ', np.std(history.history['binary_accuracy']))\n",
    "print('\\n---------------------------- Validation Accuracy ------------------------------\\n')\n",
    "print('Mean: ', np.mean(history.history['val_binary_accuracy']))\n",
    "print('Std: ', np.std(history.history['val_binary_accuracy']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6303ad8-7338-4dab-914e-d127cb8f8e2e",
   "metadata": {},
   "source": [
    ">ðŸ—’ We obtain a mean accuracy of **69%** over the validation set. We will try to improve this mean score by taking into account the numerical metadata extracted from the tweets text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa75aa6-8215-468a-bda2-aab1dc89c68a",
   "metadata": {},
   "source": [
    "### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f9ff965",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(models_path + \"model_lstm.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
